# -*- coding: utf-8 -*-
"""data_exploration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vVHFzSGG3SHZoRS7q3QMHNTCe-rbsEN5
"""

from google.colab import files
import pandas as pd

# Upload the dataset manually
uploaded = files.upload()

# Load the dataset
df = pd.read_csv("car_price_dataset.csv")

# Display first few rows
print(df.head())

# Check data types & missing values
print(df.info())

# Summary statistics
print(df.describe())

# Check for missing values
print("Missing Values:\n", df.isnull().sum())

# Check for duplicates
duplicates = df.duplicated().sum()
print(f"Number of duplicate rows: {duplicates}")

# Drop duplicates
df = df.drop_duplicates()
print(f"Dataset shape after dropping duplicates: {df.shape}")

import matplotlib.pyplot as plt
import seaborn as sns

# Set plot size
plt.figure(figsize=(15, 5))

# Plot histograms for numerical features
numeric_cols = ['Year', 'Engine_Size', 'Mileage', 'Owner_Count', 'Price']
for i, col in enumerate(numeric_cols):
    plt.subplot(1, 5, i + 1)
    sns.histplot(df[col], bins=20, kde=True)
    plt.title(f"Distribution of {col}")

plt.tight_layout()
plt.show()

from sklearn.preprocessing import LabelEncoder

# Encoding categorical features
label_encoders = {}
for col in ['Brand', 'Model', 'Fuel_Type', 'Transmission']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # Store encoders for later use

# Check the transformed dataset
print(df.head())

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

from sklearn.model_selection import train_test_split

# Define features & target
X = df.drop("Price", axis=1)  # Features
y = df["Price"]  # Target variable

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape}")
print(f"Testing set size: {X_test.shape}")

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Train the model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R² Score: {r2:.2f}")

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Define models
models = {
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Linear Regression": LinearRegression(),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
}

# Dictionary to store results
results = {}

# Train and evaluate each model
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)  # Train the model
    y_pred = model.predict(X_test)  # Make predictions

    # Calculate metrics
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store results
    results[name] = {"MAE": mae, "MSE": mse, "R² Score": r2}

# Convert results to a DataFrame for better visualization
results_df = pd.DataFrame(results).T
print("\nModel Performance Comparison:")
print(results_df)

import matplotlib.pyplot as plt

# Plot comparison of R² Scores
plt.figure(figsize=(10, 5))
sns.barplot(x=results_df.index, y=results_df["R² Score"])
plt.title("R² Score Comparison Across Models")
plt.ylabel("R² Score")
plt.xlabel("Model")
plt.ylim(0, 1)
plt.show()

# Plot comparison of MAE
plt.figure(figsize=(10, 5))
sns.barplot(x=results_df.index, y=results_df["MAE"])
plt.title("MAE Comparison Across Models (Lower is Better)")
plt.ylabel("Mean Absolute Error (MAE)")
plt.xlabel("Model")
plt.show()

import joblib
from google.colab import files

# Save the best model (XGBoost)
joblib.dump(models["XGBoost"], "car_price_xgboost.pkl")

# Download the model for PyCharm use
files.download("car_price_xgboost.pkl")